{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import collections\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device set to:  cuda\n"
     ]
    }
   ],
   "source": [
    "rtg_scale = 1000                # scale to normalize returns to go\n",
    "\n",
    "\n",
    "max_eval_ep_len = 1000      # max len of one evaluation episode\n",
    "num_eval_ep = 10            # num of evaluation episodes per iteration\n",
    "\n",
    "batch_size = 64             # training batch size\n",
    "lr = 1e-4                   # learning rate\n",
    "wt_decay = 1e-4             # weight decay\n",
    "warmup_steps = 10000        # warmup steps for lr scheduler\n",
    "\n",
    "# total updates = max_train_iters x num_updates_per_iter\n",
    "max_train_iters = 200\n",
    "num_updates_per_iter = 100\n",
    "\n",
    "context_len = 20        # K in decision transformer\n",
    "n_blocks = 3            # num of transformer blocks\n",
    "embed_dim = 128         # embedding (hidden) dim of transformer\n",
    "n_heads = 1             # num of transformer heads\n",
    "dropout_p = 0.1         # dropout probability\n",
    "\n",
    "\n",
    "\n",
    "# load data from this file\n",
    "dataset_path = f'data/M0/generated/scores.jsonl'\n",
    "\n",
    "# saves model and csv in this directory\n",
    "log_dir = \"./log/\"\n",
    "\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "\n",
    "# training and evaluation device\n",
    "device_name = 'cuda'\n",
    "device = torch.device(device_name)\n",
    "print(\"device set to: \", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "this extremely minimal GPT model is based on:\n",
    "Misha Laskin's tweet: \n",
    "https://twitter.com/MishaLaskin/status/1481767788775628801?cxt=HHwWgoCzmYD9pZApAAAA\n",
    "\n",
    "and its corresponding notebook:\n",
    "https://colab.research.google.com/drive/1NUBqyboDcGte5qAJKOl8gaJC28V_73Iv?usp=sharing\n",
    "\n",
    "the above colab has a bug while applying masked_fill which is fixed in the\n",
    "following code\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class MaskedCausalAttention(nn.Module):\n",
    "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.max_T = max_T\n",
    "\n",
    "        self.q_net = nn.Linear(h_dim, h_dim)\n",
    "        self.k_net = nn.Linear(h_dim, h_dim)\n",
    "        self.v_net = nn.Linear(h_dim, h_dim)\n",
    "\n",
    "        self.proj_net = nn.Linear(h_dim, h_dim)\n",
    "\n",
    "        self.att_drop = nn.Dropout(drop_p)\n",
    "        self.proj_drop = nn.Dropout(drop_p)\n",
    "\n",
    "        ones = torch.ones((max_T, max_T))\n",
    "        mask = torch.tril(ones).view(1, 1, max_T, max_T)\n",
    "\n",
    "        # register buffer makes sure mask does not get updated\n",
    "        # during backpropagation\n",
    "        self.register_buffer('mask',mask)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # batch size, seq length, h_dim * n_heads\n",
    "\n",
    "        N, D = self.n_heads, C // self.n_heads # N = num heads, D = attention dim\n",
    "\n",
    "        # rearrange q, k, v as (B, N, T, D)\n",
    "        q = self.q_net(x).view(B, T, N, D).transpose(1,2)\n",
    "        k = self.k_net(x).view(B, T, N, D).transpose(1,2)\n",
    "        v = self.v_net(x).view(B, T, N, D).transpose(1,2)\n",
    "\n",
    "        # weights (B, N, T, T)\n",
    "        weights = q @ k.transpose(2,3) / math.sqrt(D)\n",
    "        # causal mask applied to weights\n",
    "        weights = weights.masked_fill(self.mask[...,:T,:T] == 0, float('-inf'))\n",
    "        # normalize weights, all -inf -> 0 after softmax\n",
    "        normalized_weights = F.softmax(weights, dim=-1)\n",
    "\n",
    "        # attention (B, N, T, D)\n",
    "        attention = self.att_drop(normalized_weights @ v)\n",
    "\n",
    "        # gather heads and project (B, N, T, D) -> (B, T, N*D)\n",
    "        attention = attention.transpose(1, 2).contiguous().view(B,T,N*D)\n",
    "\n",
    "        out = self.proj_drop(self.proj_net(attention))\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, h_dim, max_T, n_heads, drop_p):\n",
    "        super().__init__()\n",
    "        self.attention = MaskedCausalAttention(h_dim, max_T, n_heads, drop_p)\n",
    "        self.mlp = nn.Sequential(\n",
    "                nn.Linear(h_dim, 4*h_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(4*h_dim, h_dim),\n",
    "                nn.Dropout(drop_p),\n",
    "            )\n",
    "        self.ln1 = nn.LayerNorm(h_dim)\n",
    "        self.ln2 = nn.LayerNorm(h_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Attention -> LayerNorm -> MLP -> LayerNorm\n",
    "        x = x + self.attention(x) # residual\n",
    "        x = self.ln1(x)\n",
    "        x = x + self.mlp(x) # residual\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    def __init__(self, state_dim, act_dim, n_blocks, h_dim, context_len, \n",
    "                 n_heads, drop_p, max_timestep=4096):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.act_dim = act_dim\n",
    "        self.h_dim = h_dim\n",
    "\n",
    "        ### transformer blocks\n",
    "        input_seq_len = 3 * context_len\n",
    "        blocks = [Block(h_dim, input_seq_len, n_heads, drop_p) for _ in range(n_blocks)]\n",
    "        self.transformer = nn.Sequential(*blocks)\n",
    "\n",
    "        ### projection heads (project to embedding)\n",
    "        self.embed_ln = nn.LayerNorm(h_dim)\n",
    "        self.embed_timestep = nn.Embedding(max_timestep, h_dim)\n",
    "        self.embed_rtg = torch.nn.Linear(1, h_dim)\n",
    "        self.embed_state = torch.nn.Linear(state_dim, h_dim)\n",
    "        \n",
    "        # # discrete actions\n",
    "        # self.embed_action = torch.nn.Embedding(act_dim, h_dim)\n",
    "        # use_action_tanh = False # False for discrete actions\n",
    "\n",
    "        # continuous actions\n",
    "        self.embed_action = torch.nn.Linear(act_dim, h_dim)\n",
    "        use_action_tanh = True # True for continuous actions\n",
    "        \n",
    "        ### prediction heads\n",
    "        self.predict_rtg = torch.nn.Linear(h_dim, 1)\n",
    "        self.predict_state = torch.nn.Linear(h_dim, state_dim)\n",
    "        self.predict_action = nn.Sequential(\n",
    "            *([nn.Linear(h_dim, act_dim)] + ([nn.Tanh()] if use_action_tanh else []))\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, timesteps, states, actions, returns_to_go):\n",
    "\n",
    "        B, T, _ = states.shape\n",
    "\n",
    "        time_embeddings = self.embed_timestep(timesteps)\n",
    "\n",
    "        # time embeddings are treated similar to positional embeddings\n",
    "        state_embeddings = self.embed_state(states) + time_embeddings\n",
    "        action_embeddings = self.embed_action(actions) + time_embeddings\n",
    "        returns_embeddings = self.embed_rtg(returns_to_go) + time_embeddings\n",
    "\n",
    "        # stack rtg, states and actions and reshape sequence as\n",
    "        # (r1, s1, a1, r2, s2, a2 ...)\n",
    "        h = torch.stack(\n",
    "            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
    "        ).permute(0, 2, 1, 3).reshape(B, 3 * T, self.h_dim)\n",
    "\n",
    "        h = self.embed_ln(h)\n",
    "        \n",
    "        # transformer and prediction\n",
    "        h = self.transformer(h)\n",
    "\n",
    "        # get h reshaped such that its size = (B x 3 x T x h_dim) and\n",
    "        # h[:, 0, t] is conditioned on r_0, s_0, a_0 ... r_t\n",
    "        # h[:, 1, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t\n",
    "        # h[:, 2, t] is conditioned on r_0, s_0, a_0 ... r_t, s_t, a_t\n",
    "        h = h.reshape(B, T, 3, self.h_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # get predictions\n",
    "        return_preds = self.predict_rtg(h[:,2])     # predict next rtg given r, s, a\n",
    "        state_preds = self.predict_state(h[:,2])    # predict next state given r, s, a\n",
    "        action_preds = self.predict_action(h[:,1])  # predict action given r, s\n",
    "    \n",
    "        return state_preds, action_preds, return_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def discount_cumsum(x, gamma):\n",
    "    disc_cumsum = np.zeros_like(x)\n",
    "    disc_cumsum[-1] = x[-1]\n",
    "    for t in reversed(range(x.shape[0]-1)):\n",
    "        disc_cumsum[t] = x[t] + gamma * disc_cumsum[t+1]\n",
    "    return disc_cumsum\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def evaluate_on_env(model, device, context_len, env, rtg_target, rtg_scale,\n",
    "                    num_eval_ep=10, max_test_ep_len=1000,\n",
    "                    state_mean=None, state_std=None, render=False):\n",
    "\n",
    "    eval_batch_size = 1  # required for forward pass\n",
    "\n",
    "    results = {}\n",
    "    total_reward = 0\n",
    "    total_timesteps = 0\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    if state_mean is None:\n",
    "        state_mean = torch.zeros((state_dim,)).to(device)\n",
    "    else:\n",
    "        state_mean = torch.from_numpy(state_mean).to(device)\n",
    "        \n",
    "    if state_std is None:\n",
    "        state_std = torch.ones((state_dim,)).to(device)\n",
    "    else:\n",
    "        state_std = torch.from_numpy(state_std).to(device)\n",
    "\n",
    "    # same as timesteps used for training the transformer\n",
    "    # also, crashes if device is passed to arange()\n",
    "    timesteps = torch.arange(start=0, end=max_test_ep_len, step=1)\n",
    "    timesteps = timesteps.repeat(eval_batch_size, 1).to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _ in range(num_eval_ep):\n",
    "\n",
    "            # zeros place holders\n",
    "            actions = torch.zeros((eval_batch_size, max_test_ep_len, act_dim),\n",
    "                                dtype=torch.float32, device=device)\n",
    "\n",
    "            states = torch.zeros((eval_batch_size, max_test_ep_len, state_dim),\n",
    "                                dtype=torch.float32, device=device)\n",
    "            \n",
    "            rewards_to_go = torch.zeros((eval_batch_size, max_test_ep_len, 1),\n",
    "                                dtype=torch.float32, device=device)\n",
    "            \n",
    "            # init episode\n",
    "            running_state = env.reset()\n",
    "            running_reward = 0\n",
    "            running_rtg = rtg_target / rtg_scale\n",
    "\n",
    "            for t in range(max_test_ep_len):\n",
    "\n",
    "                total_timesteps += 1\n",
    "\n",
    "                # add state in placeholder and normalize\n",
    "                states[0, t] = torch.from_numpy(running_state).to(device)\n",
    "                states[0, t] = (states[0, t] - state_mean) / state_std\n",
    "\n",
    "                # calcualate running rtg and add in placeholder\n",
    "                running_rtg = running_rtg - (running_reward / rtg_scale)\n",
    "                rewards_to_go[0, t] = running_rtg\n",
    "\n",
    "                if t < context_len:\n",
    "                    _, act_preds, _ = model.forward(timesteps[:,:context_len],\n",
    "                                                states[:,:context_len],\n",
    "                                                actions[:,:context_len],\n",
    "                                                rewards_to_go[:,:context_len])\n",
    "                    act = act_preds[0, t].detach()\n",
    "                else:\n",
    "                    _, act_preds, _ = model.forward(timesteps[:,t-context_len+1:t+1],\n",
    "                                                states[:,t-context_len+1:t+1],\n",
    "                                                actions[:,t-context_len+1:t+1],\n",
    "                                                rewards_to_go[:,t-context_len+1:t+1])\n",
    "                    act = act_preds[0, -1].detach()\n",
    "\n",
    "\n",
    "                running_state, running_reward, done, _ = env.step(act.cpu().numpy())\n",
    "\n",
    "                # add action in placeholder\n",
    "                actions[0, t] = act\n",
    "\n",
    "                total_reward += running_reward\n",
    "\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "    results['eval/avg_reward'] = total_reward / num_eval_ep\n",
    "    results['eval/avg_ep_len'] = total_timesteps / num_eval_ep\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, '{'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## check data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# load dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(dataset_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 5\u001b[0m     trajectories \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m min_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m\n\u001b[1;32m      8\u001b[0m states \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '{'."
     ]
    }
   ],
   "source": [
    "## check data\n",
    "\n",
    "# load dataset\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    trajectories = pickle.load(f)\n",
    "\n",
    "min_len = 10**4\n",
    "states = []\n",
    "for traj in trajectories:\n",
    "    min_len = min(min_len, traj['observations'].shape[0])\n",
    "    states.append(traj['observations'])\n",
    "\n",
    "# used for input normalization\n",
    "states = np.concatenate(states, axis=0)\n",
    "state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "print(dataset_path)\n",
    "print(\"num of trajectories in dataset: \", len(trajectories))\n",
    "print(\"minimum trajectory length in dataset: \", min_len)\n",
    "print(\"state mean: \", state_mean.tolist())\n",
    "print(\"state std: \", state_std.tolist())\n",
    "\n",
    "\n",
    "## check if info is correct\n",
    "print(\"is state mean info correct: \", state_mean.tolist() == D4RL_DATASET_STATS[env_d4rl_name]['state_mean'])\n",
    "print(\"is state std info correct: \", state_std.tolist() == D4RL_DATASET_STATS[env_d4rl_name]['state_std'])\n",
    "\n",
    "\n",
    "assert state_mean.tolist() == D4RL_DATASET_STATS[env_d4rl_name]['state_mean']\n",
    "assert state_std.tolist() == D4RL_DATASET_STATS[env_d4rl_name]['state_std']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
