{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Head2Head Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_m0 = r\"D:\\Umich\\2024Winter\\EECS 598\\Project\\data\\test set\\responses_M0.jsonl\"\n",
    "filepath_m1 = r\"D:\\Umich\\2024Winter\\EECS 598\\Project\\data\\test set\\responses_M1.jsonl\"\n",
    "filepath_m2 = r\"D:\\Umich\\2024Winter\\EECS 598\\Project\\data\\test set\\responses_M2.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(filepath_m0) as reader:\n",
    "    data_m0 = [obj for obj in reader]\n",
    "with jsonlines.open(filepath_m1) as reader:\n",
    "    data_m1 = [obj for obj in reader]\n",
    "with jsonlines.open(filepath_m2) as reader:\n",
    "    data_m2 = [obj for obj in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort data_m1 by prompt id\n",
    "data_m0 = sorted(data_m0, key=lambda x: x['prompt_id'])\n",
    "data_m1 = sorted(data_m1, key=lambda x: x['prompt_id'])\n",
    "data_m2 = sorted(data_m2, key=lambda x: x['prompt_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srlm_prompt = \"\"\"Review the user’s question and the corresponding response using the additive 5-point\n",
    "    scoring system described below. \n",
    "\n",
    "    The user's question is between <question> and </question>\n",
    "    The response of the AI Assistant is between <response> and </response>\n",
    "\n",
    "    Points are accumulated based on the satisfaction of each\n",
    "    criterion:\n",
    "    - Add 1 point if the response is relevant and provides some information related to\n",
    "    the user’s inquiry, even if it is incomplete or contains some irrelevant content.\n",
    "    - Add another point if the response addresses a substantial portion of the user’s question,\n",
    "    but does not completely resolve the query or provide a direct answer.\n",
    "    - Award a third point if the response answers the basic elements of the user’s question in a\n",
    "    useful way, regardless of whether it seems to have been written by an AI Assistant or if it\n",
    "    has elements typically found in blogs or search results.\n",
    "    - Grant a fourth point if the response is clearly written from an AI Assistant’s perspective,\n",
    "    addressing the user’s question directly and comprehensively, and is well-organized and\n",
    "    helpful, even if there is slight room for improvement in clarity, conciseness or focus.\n",
    "    - Bestow a fifth point for a response that is impeccably tailored to the user’s question\n",
    "    by an AI Assistant, without extraneous information, reflecting expert knowledge, and\n",
    "    demonstrating a high-quality, engaging, and insightful answer.\n",
    "    - If the response repeats itself or is not concise and to the point, score the response 0.\n",
    "\n",
    "    <question>{prompt}</question>\n",
    "    <response>{response}</response>\n",
    "\n",
    "    After examining the user’s instruction and the response:\n",
    "    - output the score of the evaluation using this exact format: \"score: <total points>\", where <total points> is between 0 and 5\n",
    "    - Briefly justify your total score, up to 100 words.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slrm_prompt = '''\n",
    "Task Description:\n",
    "You are a virtual judge tasked with evaluating responses to some questions. Your role is to assess the quality and relevance of each answer and decide which is the better one.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Review the Question: Begin by carefully examining the question provided. Understand the context, elements, and any detail that may influence the answers to the questions.\n",
    "Evaluate the Answers:\n",
    "For each question, read the two provided answers.\n",
    "Consider the following criteria in your evaluation:\n",
    "Relevance: How well does the answer respond to the question?\n",
    "Accuracy: Is the information in the answer correct and well-supported?\n",
    "Completeness: Does the answer cover the necessary aspects of the question without being overly simplistic or unnecessarily complex?\n",
    "Clarity: How clear and understandable is the explanation or argument in the answer?\n",
    "\n",
    "Prompt: <question>{prompt}</question>\n",
    "\n",
    "Response1: <response>{response1}</response>\n",
    "\n",
    "Response2: <response>{response2}</response>\n",
    "\n",
    "Output Format:\n",
    "Judgement output: If the response1 is better than response2, output 1; if response2 is better than response1, output 0. Report in the exact format: \"judgement: <1,0>\". \n",
    "Reasoning: Briefly explain why you chose the better response, up to 50 words.\n",
    "\n",
    "GO GPT! You can do it!\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M0 V.S. M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'API_KEY' # replace with your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "compare_m0_m1 = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    for j in range(4):\n",
    "        compare_dict = {}\n",
    "        compare_dict['prompt_id'] = data_m1[i]['prompt_id']\n",
    "        compare_dict['prompt'] = data_m1[i]['prompt']\n",
    "        compare_dict['response_m0'] = data_m0[i]['completion'][str(j)]\n",
    "        compare_dict['response_m1'] = data_m1[i]['completion'][str(j)]\n",
    "        time.sleep(0.5)\n",
    "        user_input = slrm_prompt.format(prompt=data_m1[i]['prompt'], \n",
    "                                        response1=data_m0[i]['completion'][str(j)], \n",
    "                                        response2=data_m1[i]['completion'][str(j)])\n",
    "        stream = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \n",
    "                    \"content\": user_input}],\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        output_dict = {}\n",
    "        output_id = 0  # Or use any other method to generate a unique ID for each output\n",
    "        output = []\n",
    "\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                # Save the output in the dictionary using a unique ID\n",
    "                output.append(chunk.choices[0].delta.content)\n",
    "        \n",
    "        output_str = ''.join(output)\n",
    "        compare_dict['output'] = output_str\n",
    "        compare_m0_m1.append(compare_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_m1_m0 = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    for j in range(4):\n",
    "        compare_dict = {}\n",
    "        compare_dict['prompt_id'] = data_m1[i]['prompt_id']\n",
    "        compare_dict['prompt'] = data_m1[i]['prompt']\n",
    "        compare_dict['response_m0'] = data_m0[i]['completion'][str(j)]\n",
    "        compare_dict['response_m1'] = data_m1[i]['completion'][str(j)]\n",
    "        time.sleep(0.5)\n",
    "        user_input = slrm_prompt.format(prompt=data_m1[i]['prompt'], \n",
    "                                        response1=data_m1[i]['completion'][str(j)], \n",
    "                                        response2=data_m0[i]['completion'][str(j)])\n",
    "        stream = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \n",
    "                    \"content\": user_input}],\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        output_dict = {}\n",
    "        output_id = 0  # Or use any other method to generate a unique ID for each output\n",
    "        output = []\n",
    "\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                # Save the output in the dictionary using a unique ID\n",
    "                output.append(chunk.choices[0].delta.content)\n",
    "        \n",
    "        output_str = ''.join(output)\n",
    "        compare_dict['output'] = output_str\n",
    "        compare_m1_m0.append(compare_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(r\"D:\\Umich\\2024Winter\\EECS 598\\Project\\data\\test set\\compare_m1_m0.jsonl\", mode='w') as writer:\n",
    "    writer.write_all(compare_m1_m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read jsonl file\n",
    "with jsonlines.open(r\"D:\\Umich\\2024Winter\\EECS 598\\Project\\data\\test set\\compare_m0_m1.jsonl\") as reader:\n",
    "    compare_m0_m1 = [obj for obj in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_m0_m1 = compare_m0_m1[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_m0_m1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_m1_m0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine comparison results\n",
    "import re\n",
    "\n",
    "m0_m1_dict = {\n",
    "    'prompt_id': [],\n",
    "    'prompt': [],\n",
    "    'response_m0': [],\n",
    "    'response_m1': [],\n",
    "    'score':[]\n",
    "}\n",
    "for i in tqdm(range(400)):\n",
    "    m0_m1_dict['prompt_id'].append(compare_m0_m1[i]['prompt_id'])\n",
    "    m0_m1_dict['prompt'].append(compare_m0_m1[i]['prompt'])\n",
    "    m0_m1_dict['response_m0'].append(compare_m0_m1[i]['response_m0'])\n",
    "    m0_m1_dict['response_m1'].append(compare_m0_m1[i]['response_m1'])\n",
    "    judgement_search_order = re.search(r'Judgement\\s*:\\s*(\\d+)', compare_m0_m1[i]['output'])\n",
    "    score_order = int(judgement_search_order.group(1)) if judgement_search_order else None\n",
    "    judgement_search_reverse = re.search(r'Judgement\\s*:\\s*(\\d+)', compare_m1_m0[i]['output'])\n",
    "    score_order_reverse = int(judgement_search_reverse.group(1)) if judgement_search_reverse else None\n",
    "    if score_order is not None and score_order_reverse is not None:\n",
    "        score = score_order - score_order_reverse\n",
    "    elif score_order is not None:\n",
    "        if score_order == 0: # (0, None) -> m0 worse\n",
    "            score = -1\n",
    "        else: # (1, None) -> m0 better\n",
    "            score = 1\n",
    "    elif score_order_reverse is not None: # (None, x)\n",
    "        if score_order_reverse == 0: # (None, 0) -> m0 better\n",
    "            score = 1\n",
    "        else: # (None, 1) -> m0 worse\n",
    "            score = -1\n",
    "    else:\n",
    "        score = None\n",
    "    m0_m1_dict['score'].append(score)\n",
    "\n",
    "m0_m1_df = pd.DataFrame(m0_m1_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_m1_df['score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m0_m1_df.to_csv(r\"D:\\Umich\\2024Winter\\EECS 598\\Project\\data\\test set\\m0_m1_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M1 V.S. M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first round (M1,M2)\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "compare_m1_m2 = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    for j in range(4):\n",
    "        compare_dict = {}\n",
    "        compare_dict['prompt_id'] = data_m1[i]['prompt_id']\n",
    "        compare_dict['prompt'] = data_m1[i]['prompt']\n",
    "        compare_dict['response_m1'] = data_m1[i]['completion'][str(j)]\n",
    "        compare_dict['response_m2'] = data_m2[i]['completion'][str(j)]\n",
    "        time.sleep(0.5)\n",
    "        user_input = slrm_prompt.format(prompt=data_m1[i]['prompt'], \n",
    "                                        response1=data_m1[i]['completion'][str(j)], \n",
    "                                        response2=data_m2[i]['completion'][str(j)])\n",
    "        stream = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \n",
    "                    \"content\": user_input}],\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        output_dict = {}\n",
    "        output_id = 0  # Or use any other method to generate a unique ID for each output\n",
    "        output = []\n",
    "\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                # Save the output in the dictionary using a unique ID\n",
    "                output.append(chunk.choices[0].delta.content)\n",
    "        \n",
    "        output_str = ''.join(output)\n",
    "        compare_dict['output'] = output_str\n",
    "        compare_m1_m2.append(compare_dict)\n",
    "\n",
    "with jsonlines.open(r\"D:\\Umich\\2024Winter\\EECS 598\\Project\\data\\test set\\compare_m1_m2.jsonl\", mode='w') as writer:\n",
    "    writer.write_all(compare_m1_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second round (M2,M1)\n",
    "\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "compare_m2_m1 = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    for j in range(4):\n",
    "        compare_dict = {}\n",
    "        compare_dict['prompt_id'] = data_m1[i]['prompt_id']\n",
    "        compare_dict['prompt'] = data_m1[i]['prompt']\n",
    "        compare_dict['response_m1'] = data_m1[i]['completion'][str(j)]\n",
    "        compare_dict['response_m2'] = data_m2[i]['completion'][str(j)]\n",
    "        time.sleep(0.5)\n",
    "        user_input = slrm_prompt.format(prompt=data_m1[i]['prompt'], \n",
    "                                        response1=data_m2[i]['completion'][str(j)], \n",
    "                                        response2=data_m1[i]['completion'][str(j)])\n",
    "        stream = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \n",
    "                    \"content\": user_input}],\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        output_dict = {}\n",
    "        output_id = 0  # Or use any other method to generate a unique ID for each output\n",
    "        output = []\n",
    "\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                # Save the output in the dictionary using a unique ID\n",
    "                output.append(chunk.choices[0].delta.content)\n",
    "        \n",
    "        output_str = ''.join(output)\n",
    "        compare_dict['output'] = output_str\n",
    "        compare_m2_m1.append(compare_dict)\n",
    "\n",
    "with jsonlines.open(r\"D:\\Umich\\2024Winter\\EECS 598\\Project\\data\\test set\\compare_m2_m1.jsonl\", mode='w') as writer:\n",
    "    writer.write_all(compare_m2_m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine comparison results\n",
    "import re\n",
    "\n",
    "m1_m2_dict = {\n",
    "    'prompt_id': [],\n",
    "    'prompt': [],\n",
    "    'response_m1': [],\n",
    "    'response_m2': [],\n",
    "    'score':[]\n",
    "}\n",
    "for i in tqdm(range(400)):\n",
    "    m1_m2_dict['prompt_id'].append(compare_m1_m2[i]['prompt_id'])\n",
    "    m1_m2_dict['prompt'].append(compare_m1_m2[i]['prompt'])\n",
    "    m1_m2_dict['response_m1'].append(compare_m1_m2[i]['response_m1'])\n",
    "    m1_m2_dict['response_m2'].append(compare_m1_m2[i]['response_m2'])\n",
    "    judgement_search_order = re.search(r'Judgement\\s*:\\s*(\\d+)', compare_m1_m2[i]['output'])\n",
    "    score_order = int(judgement_search_order.group(1)) if judgement_search_order else None\n",
    "    judgement_search_reverse = re.search(r'Judgement\\s*:\\s*(\\d+)', compare_m2_m1[i]['output'])\n",
    "    score_order_reverse = int(judgement_search_reverse.group(1)) if judgement_search_reverse else None\n",
    "    if score_order is not None and score_order_reverse is not None:\n",
    "        score = score_order - score_order_reverse\n",
    "    elif score_order is not None:\n",
    "        if score_order == 0: # (0, None) -> m1 worse\n",
    "            score = -1\n",
    "        else: # (1, None) -> m1 better\n",
    "            score = 1\n",
    "    elif score_order_reverse is not None: # (None, x)\n",
    "        if score_order_reverse == 0: # (None, 0) -> m1 better\n",
    "            score = 1\n",
    "        else: # (None, 1) -> m1 worse\n",
    "            score = -1\n",
    "    else:\n",
    "        score = None\n",
    "    m1_m2_dict['score'].append(score)\n",
    "\n",
    "m1_m2_df = pd.DataFrame(m1_m2_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_m2_df['score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_m2_df.to_csv(r\"D:\\Umich\\2024Winter\\EECS 598\\Project\\data\\test set\\m1_m2_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M0 V.S. M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first round (M0,M2)\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "compare_m0_m2 = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    for j in range(4):\n",
    "        compare_dict = {}\n",
    "        compare_dict['prompt_id'] = data_m0[i]['prompt_id']\n",
    "        compare_dict['prompt'] = data_m0[i]['prompt']\n",
    "        compare_dict['response_m0'] = data_m0[i]['completion'][str(j)]\n",
    "        compare_dict['response_m2'] = data_m2[i]['completion'][str(j)]\n",
    "        time.sleep(0.5)\n",
    "        user_input = slrm_prompt.format(prompt=data_m0[i]['prompt'], \n",
    "                                        response1=data_m0[i]['completion'][str(j)], \n",
    "                                        response2=data_m2[i]['completion'][str(j)])\n",
    "        stream = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[{\"role\": \"user\", \n",
    "                    \"content\": user_input}],\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        output_dict = {}\n",
    "        output_id = 0  # Or use any other method to generate a unique ID for each output\n",
    "        output = []\n",
    "\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                # Save the output in the dictionary using a unique ID\n",
    "                output.append(chunk.choices[0].delta.content)\n",
    "        \n",
    "        output_str = ''.join(output)\n",
    "        compare_dict['output'] = output_str\n",
    "        compare_m0_m2.append(compare_dict)\n",
    "\n",
    "with jsonlines.open(r\"D:\\Umich\\2024Winter\\EECS 598\\Project\\data\\test set\\compare_m0_m2.jsonl\", mode='w') as writer:\n",
    "    writer.write_all(compare_m0_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second round (M2,M0)\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "compare_m2_m0 = []\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    for j in range(4):\n",
    "        compare_dict = {}\n",
    "        compare_dict['prompt_id'] = data_m0[i]['prompt_id']\n",
    "        compare_dict['prompt'] = data_m0[i]['prompt']\n",
    "        compare_dict['response_m0'] = data_m0[i]['completion'][str(j)]\n",
    "        compare_dict['response_m2'] = data_m2[i]['completion'][str(j)]\n",
    "        time.sleep(0.5)\n",
    "        user_input = slrm_prompt.format(prompt=data_m0[i]['prompt'], \n",
    "                                        response1=data_m2[i]['completion'][str(j)], \n",
    "                                        response2=data_m0[i]['completion'][str(j)])\n",
    "        stream = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[{\"role\": \"user\", \n",
    "                    \"content\": user_input}],\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        output_dict = {}\n",
    "        output_id = 0  # Or use any other method to generate a unique ID for each output\n",
    "        output = []\n",
    "\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                # Save the output in the dictionary using a unique ID\n",
    "                output.append(chunk.choices[0].delta.content)\n",
    "        \n",
    "        output_str = ''.join(output)\n",
    "        compare_dict['output'] = output_str\n",
    "        compare_m2_m0.append(compare_dict)\n",
    "\n",
    "with jsonlines.open(r\"D:\\Umich\\2024Winter\\EECS 598\\Project\\data\\test set\\compare_m2_m0.jsonl\", mode='w') as writer:\n",
    "    writer.write_all(compare_m2_m0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_m0_m2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
